{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cbd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def EEGNet(nb_classes, Chans=22, Samples=1000, dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25):\n",
    "    input1 = Input(shape=(Chans, Samples, 1))\n",
    "\n",
    "    block1 = Conv2D(F1, (1, kernLength), padding='same',\n",
    "                    input_shape=(Chans, Samples, 1),\n",
    "                    use_bias=False)(input1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False,\n",
    "                             depth_multiplier=D,\n",
    "                             depthwise_regularizer=l2(0.001),\n",
    "                             padding='valid')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = AveragePooling2D((1, 4))(block1)\n",
    "    block1 = Dropout(dropoutRate)(block1)\n",
    "\n",
    "    block2 = SeparableConv2D(F2, (1, 16),\n",
    "                             use_bias=False, padding='same')(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((1, 8))(block2)\n",
    "    block2 = Dropout(dropoutRate)(block2)\n",
    "\n",
    "    flatten = Flatten()(block2)\n",
    "\n",
    "    dense = Dense(nb_classes, activation='softmax',\n",
    "                  kernel_regularizer=l2(0.001))(flatten)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"E:\\sem8\\Final\\NEW_TRY\\BCICIV_2a_all_patients_normalized1.csv\")\n",
    "\n",
    "# Extract EEG channels\n",
    "eeg_channels = [col for col in df.columns if col.startswith('EEG-')]\n",
    "grouped = df.groupby(['patient', 'epoch'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "label_map = {'left': 0, 'right': 1, 'foot': 2, 'tongue': 3}\n",
    "\n",
    "for (patient, epoch), group in grouped:\n",
    "    group_sorted = group.sort_values(by='time')\n",
    "    data = group_sorted[eeg_channels].values\n",
    "    if data.shape[0] < 1000:  # pad if needed\n",
    "        data = np.pad(data, ((0, 1000 - data.shape[0]), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        data = data[:1000]  # crop\n",
    "    X.append(data)\n",
    "    label = group_sorted['label'].iloc[0]\n",
    "    y.append(label_map[label])\n",
    "\n",
    "X = np.array(X)  # shape: (n_trials, time_steps, 22)\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Reshape for EEGNet: (n_samples, channels, time, 1)\n",
    "X = np.transpose(X, (0, 2, 1))  # (samples, channels, time)\n",
    "X = X[..., np.newaxis]          # (samples, channels, time, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y.argmax(1), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e250bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python365\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 120ms/step - accuracy: 0.2448 - loss: 1.5828 - val_accuracy: 0.2347 - val_loss: 1.3970\n",
      "Epoch 2/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.2614 - loss: 1.4475 - val_accuracy: 0.2500 - val_loss: 1.3962\n",
      "Epoch 3/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.2780 - loss: 1.4272 - val_accuracy: 0.2806 - val_loss: 1.3936\n",
      "Epoch 4/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.2712 - loss: 1.4014 - val_accuracy: 0.2832 - val_loss: 1.3895\n",
      "Epoch 5/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.3056 - loss: 1.3966 - val_accuracy: 0.3163 - val_loss: 1.3861\n",
      "Epoch 6/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.3174 - loss: 1.3818 - val_accuracy: 0.3010 - val_loss: 1.3800\n",
      "Epoch 7/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3347 - loss: 1.3667 - val_accuracy: 0.3444 - val_loss: 1.3721\n",
      "Epoch 8/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.3222 - loss: 1.3703 - val_accuracy: 0.3571 - val_loss: 1.3648\n",
      "Epoch 9/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3228 - loss: 1.3738 - val_accuracy: 0.3291 - val_loss: 1.3547\n",
      "Epoch 10/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.3481 - loss: 1.3604 - val_accuracy: 0.3393 - val_loss: 1.3523\n",
      "Epoch 11/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.3355 - loss: 1.3539 - val_accuracy: 0.3418 - val_loss: 1.3462\n",
      "Epoch 12/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.3733 - loss: 1.3281 - val_accuracy: 0.3597 - val_loss: 1.3496\n",
      "Epoch 13/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.3589 - loss: 1.3555 - val_accuracy: 0.3622 - val_loss: 1.3311\n",
      "Epoch 14/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.3367 - loss: 1.3511 - val_accuracy: 0.3469 - val_loss: 1.3333\n",
      "Epoch 15/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.3543 - loss: 1.3271 - val_accuracy: 0.3469 - val_loss: 1.3265\n",
      "Epoch 16/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3437 - loss: 1.3600 - val_accuracy: 0.3163 - val_loss: 1.3322\n",
      "Epoch 17/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.3718 - loss: 1.3246 - val_accuracy: 0.3622 - val_loss: 1.3232\n",
      "Epoch 18/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.3885 - loss: 1.3161 - val_accuracy: 0.3878 - val_loss: 1.3085\n",
      "Epoch 19/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3710 - loss: 1.3336 - val_accuracy: 0.3597 - val_loss: 1.3141\n",
      "Epoch 20/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.3540 - loss: 1.3274 - val_accuracy: 0.3597 - val_loss: 1.3080\n",
      "Epoch 21/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4077 - loss: 1.3050 - val_accuracy: 0.3827 - val_loss: 1.3070\n",
      "Epoch 22/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 128ms/step - accuracy: 0.4089 - loss: 1.2947 - val_accuracy: 0.3724 - val_loss: 1.3000\n",
      "Epoch 23/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.3849 - loss: 1.3179 - val_accuracy: 0.3750 - val_loss: 1.2989\n",
      "Epoch 24/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4168 - loss: 1.2770 - val_accuracy: 0.3724 - val_loss: 1.2911\n",
      "Epoch 25/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.3791 - loss: 1.2972 - val_accuracy: 0.3929 - val_loss: 1.2920\n",
      "Epoch 26/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4094 - loss: 1.2862 - val_accuracy: 0.4031 - val_loss: 1.2846\n",
      "Epoch 27/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.3757 - loss: 1.2985 - val_accuracy: 0.3776 - val_loss: 1.2800\n",
      "Epoch 28/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4081 - loss: 1.2787 - val_accuracy: 0.3827 - val_loss: 1.2761\n",
      "Epoch 29/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4049 - loss: 1.2784 - val_accuracy: 0.4209 - val_loss: 1.2887\n",
      "Epoch 30/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4112 - loss: 1.2747 - val_accuracy: 0.3827 - val_loss: 1.2803\n",
      "Epoch 31/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4581 - loss: 1.2548 - val_accuracy: 0.4184 - val_loss: 1.2843\n",
      "Epoch 32/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4166 - loss: 1.2573 - val_accuracy: 0.4209 - val_loss: 1.2810\n",
      "Epoch 33/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4144 - loss: 1.2681 - val_accuracy: 0.4464 - val_loss: 1.2615\n",
      "Epoch 34/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4533 - loss: 1.2465 - val_accuracy: 0.4439 - val_loss: 1.2608\n",
      "Epoch 35/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4420 - loss: 1.2307 - val_accuracy: 0.4515 - val_loss: 1.2554\n",
      "Epoch 36/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4205 - loss: 1.2553 - val_accuracy: 0.4490 - val_loss: 1.2464\n",
      "Epoch 37/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4421 - loss: 1.2426 - val_accuracy: 0.4745 - val_loss: 1.2436\n",
      "Epoch 38/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.4160 - loss: 1.2500 - val_accuracy: 0.4668 - val_loss: 1.2430\n",
      "Epoch 39/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4608 - loss: 1.2229 - val_accuracy: 0.4694 - val_loss: 1.2422\n",
      "Epoch 40/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4404 - loss: 1.2293 - val_accuracy: 0.4617 - val_loss: 1.2389\n",
      "Epoch 41/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4137 - loss: 1.2236 - val_accuracy: 0.4413 - val_loss: 1.2353\n",
      "Epoch 42/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4636 - loss: 1.2150 - val_accuracy: 0.4821 - val_loss: 1.2325\n",
      "Epoch 43/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 0.4570 - loss: 1.2239 - val_accuracy: 0.4413 - val_loss: 1.2536\n",
      "Epoch 44/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4687 - loss: 1.2179 - val_accuracy: 0.4439 - val_loss: 1.2348\n",
      "Epoch 45/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4444 - loss: 1.2316 - val_accuracy: 0.4847 - val_loss: 1.2229\n",
      "Epoch 46/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4624 - loss: 1.2081 - val_accuracy: 0.4668 - val_loss: 1.2188\n",
      "Epoch 47/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4412 - loss: 1.2197 - val_accuracy: 0.4949 - val_loss: 1.2137\n",
      "Epoch 48/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.4873 - loss: 1.1811 - val_accuracy: 0.4464 - val_loss: 1.2514\n",
      "Epoch 49/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4508 - loss: 1.1995 - val_accuracy: 0.4745 - val_loss: 1.2307\n",
      "Epoch 50/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4528 - loss: 1.2177 - val_accuracy: 0.4566 - val_loss: 1.2262\n",
      "Epoch 51/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.4693 - loss: 1.1878 - val_accuracy: 0.4745 - val_loss: 1.2071\n",
      "Epoch 52/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4527 - loss: 1.2109 - val_accuracy: 0.4821 - val_loss: 1.2146\n",
      "Epoch 53/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4624 - loss: 1.1849 - val_accuracy: 0.4898 - val_loss: 1.2036\n",
      "Epoch 54/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4707 - loss: 1.1919 - val_accuracy: 0.4872 - val_loss: 1.1994\n",
      "Epoch 55/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.4595 - loss: 1.1937 - val_accuracy: 0.4872 - val_loss: 1.2015\n",
      "Epoch 56/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4689 - loss: 1.1972 - val_accuracy: 0.4796 - val_loss: 1.2094\n",
      "Epoch 57/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4624 - loss: 1.1875 - val_accuracy: 0.4668 - val_loss: 1.1974\n",
      "Epoch 58/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.5009 - loss: 1.1539 - val_accuracy: 0.4821 - val_loss: 1.1958\n",
      "Epoch 59/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4931 - loss: 1.1633 - val_accuracy: 0.4923 - val_loss: 1.1777\n",
      "Epoch 60/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4684 - loss: 1.2079 - val_accuracy: 0.4872 - val_loss: 1.2022\n",
      "Epoch 61/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4838 - loss: 1.1714 - val_accuracy: 0.4719 - val_loss: 1.1951\n",
      "Epoch 62/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.5068 - loss: 1.1863 - val_accuracy: 0.5102 - val_loss: 1.1767\n",
      "Epoch 63/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4641 - loss: 1.1767 - val_accuracy: 0.4949 - val_loss: 1.1831\n",
      "Epoch 64/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4958 - loss: 1.1515 - val_accuracy: 0.4847 - val_loss: 1.1967\n",
      "Epoch 65/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4672 - loss: 1.1965 - val_accuracy: 0.4847 - val_loss: 1.1844\n",
      "Epoch 66/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4695 - loss: 1.1928 - val_accuracy: 0.4847 - val_loss: 1.1770\n",
      "Epoch 67/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4813 - loss: 1.1746 - val_accuracy: 0.5026 - val_loss: 1.1795\n",
      "Epoch 68/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5153 - loss: 1.1403 - val_accuracy: 0.4847 - val_loss: 1.1811\n",
      "Epoch 69/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4754 - loss: 1.1673 - val_accuracy: 0.5077 - val_loss: 1.1766\n",
      "Epoch 70/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4732 - loss: 1.1753 - val_accuracy: 0.4770 - val_loss: 1.1833\n",
      "Epoch 71/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.4754 - loss: 1.1651 - val_accuracy: 0.4923 - val_loss: 1.1756\n",
      "Epoch 72/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.4969 - loss: 1.1512 - val_accuracy: 0.5026 - val_loss: 1.1726\n",
      "Epoch 73/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4980 - loss: 1.1572 - val_accuracy: 0.4770 - val_loss: 1.1873\n",
      "Epoch 74/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.5359 - loss: 1.1296 - val_accuracy: 0.4949 - val_loss: 1.1613\n",
      "Epoch 75/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4990 - loss: 1.1429 - val_accuracy: 0.4949 - val_loss: 1.1691\n",
      "Epoch 76/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4848 - loss: 1.1655 - val_accuracy: 0.4923 - val_loss: 1.1654\n",
      "Epoch 77/100\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4985 - loss: 1.1623 - val_accuracy: 0.4796 - val_loss: 1.1717\n",
      "Test Accuracy: 52.04%\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet(nb_classes=4, Chans=22, Samples=1000)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77264141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save model to file\n",
    "model.save(\"eeg_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb461ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step\n",
      "Raw predictions:\n",
      " [[0.4606969  0.13146338 0.2579977  0.149842  ]\n",
      " [0.376305   0.23660183 0.16584842 0.2212447 ]\n",
      " [0.24468043 0.47660932 0.15276816 0.12594202]\n",
      " [0.38998693 0.11366347 0.10394442 0.39240515]\n",
      " [0.57750547 0.17519674 0.15199094 0.09530683]\n",
      " [0.2361813  0.41088712 0.22748698 0.1254446 ]\n",
      " [0.29636675 0.34660792 0.23757693 0.11944833]\n",
      " [0.09595322 0.52144843 0.06109606 0.32150227]\n",
      " [0.32359543 0.12199177 0.18249795 0.3719149 ]\n",
      " [0.19896689 0.46472397 0.2389148  0.09739432]]\n",
      "Predicted classes: [0 0 1 3 0 1 1 1 3 1]\n",
      "True classes:      [0 2 1 3 0 1 2 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Select the first 10 samples from X_test\n",
    "sample_data = X_test[:10]\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(sample_data)\n",
    "\n",
    "# Print the raw prediction outputs\n",
    "print(\"Raw predictions:\\n\", predictions)\n",
    "\n",
    "# Optionally, print the predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "\n",
    "# And if you want to compare with true labels:\n",
    "true_classes = np.argmax(y_test[:10], axis=1)\n",
    "print(\"True classes:     \", true_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ---------------------- Filtering Functions ----------------------\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    return butter(order, [low, high], btype='band')\n",
    "\n",
    "def bandpass_filter_data(data, lowcut=4, highcut=40, fs=250):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# ---------------------- EEGNet Model ----------------------\n",
    "def EEGNet(nb_classes, Chans=22, Samples=1000, dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16):\n",
    "    input1 = Input(shape=(Chans, Samples, 1))\n",
    "\n",
    "    block1 = Conv2D(F1, (1, kernLength), padding='same', use_bias=False)(input1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False, depth_multiplier=D, depthwise_regularizer=l2(0.001), padding='valid')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = AveragePooling2D((1, 4))(block1)\n",
    "    block1 = Dropout(dropoutRate)(block1)\n",
    "\n",
    "    block2 = SeparableConv2D(F2, (1, 16), use_bias=False, padding='same')(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((1, 8))(block2)\n",
    "    block2 = Dropout(dropoutRate)(block2)\n",
    "\n",
    "    flatten = Flatten()(block2)\n",
    "    dense = Dense(nb_classes, activation='softmax', kernel_regularizer=l2(0.001))(flatten)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense)\n",
    "    return model\n",
    "\n",
    "# ---------------------- Load and Preprocess Data ----------------------\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"E:\\sem8\\Final\\NEW_TRY\\BCICIV_2a_all_patients_normalized1.csv\")\n",
    "\n",
    "eeg_channels = [col for col in df.columns if col.startswith('EEG-')]\n",
    "grouped = df.groupby(['patient', 'epoch'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "label_map = {'left': 0, 'right': 1, 'foot': 2, 'tongue': 3}\n",
    "fs = 250  # Sampling frequency\n",
    "target_length = 1000\n",
    "\n",
    "for (patient, epoch), group in grouped:\n",
    "    group_sorted = group.sort_values(by='time')\n",
    "    data = group_sorted[eeg_channels].values  # shape: (time_steps, 22)\n",
    "\n",
    "    # Pad or crop\n",
    "    if data.shape[0] < target_length:\n",
    "        data = np.pad(data, ((0, target_length - data.shape[0]), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        data = data[:target_length]\n",
    "\n",
    "    # Z-score normalization per channel\n",
    "    data = (data - np.mean(data, axis=0)) / (np.std(data, axis=0) + 1e-6)\n",
    "\n",
    "    X.append(data)\n",
    "    y.append(label_map[group_sorted['label'].iloc[0]])\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(X)  # shape: (n_trials, time_steps, channels)\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Reshape for EEGNet: (samples, channels, time, 1)\n",
    "X = np.transpose(X, (0, 2, 1))  # (samples, channels, time)\n",
    "X = X[..., np.newaxis]         # (samples, channels, time, 1)\n",
    "\n",
    "# ---------------------- Train-Test Split ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=np.argmax(y, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------- Compute Class Weights ----------------------\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# ---------------------- Train EEGNet ----------------------\n",
    "model = EEGNet(nb_classes=4, Chans=22, Samples=1000)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=150,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------- Evaluate ----------------------\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Test Accuracy: {acc * 100:.2f}%\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540eaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_sorted['time'].diff().mean()  # Should be ~0.004 → 1/250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.2490 - loss: 1.5070 - val_accuracy: 0.2730 - val_loss: 1.3953 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.2535 - loss: 1.4605 - val_accuracy: 0.2602 - val_loss: 1.3944 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.2317 - loss: 1.4579 - val_accuracy: 0.2500 - val_loss: 1.3941 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.2659 - loss: 1.4266 - val_accuracy: 0.2219 - val_loss: 1.3952 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2687 - loss: 1.4245 - val_accuracy: 0.2219 - val_loss: 1.3977 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2650 - loss: 1.4050 - val_accuracy: 0.2602 - val_loss: 1.3945 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.2673 - loss: 1.3999 - val_accuracy: 0.2653 - val_loss: 1.3907 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.2698 - loss: 1.3935 - val_accuracy: 0.2398 - val_loss: 1.3970 - learning_rate: 0.0010\n",
      "Epoch 9/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2763 - loss: 1.4052 - val_accuracy: 0.3189 - val_loss: 1.3840 - learning_rate: 0.0010\n",
      "Epoch 10/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.2668 - loss: 1.4048 - val_accuracy: 0.3061 - val_loss: 1.3858 - learning_rate: 0.0010\n",
      "Epoch 11/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.2573 - loss: 1.4038 - val_accuracy: 0.2985 - val_loss: 1.3867 - learning_rate: 0.0010\n",
      "Epoch 12/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.3100 - loss: 1.3755 - val_accuracy: 0.2551 - val_loss: 1.3900 - learning_rate: 0.0010\n",
      "Epoch 13/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.2878 - loss: 1.3878 - val_accuracy: 0.2857 - val_loss: 1.3825 - learning_rate: 0.0010\n",
      "Epoch 14/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2802 - loss: 1.3947 - val_accuracy: 0.3342 - val_loss: 1.3785 - learning_rate: 0.0010\n",
      "Epoch 15/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3127 - loss: 1.3747 - val_accuracy: 0.2679 - val_loss: 1.3847 - learning_rate: 0.0010\n",
      "Epoch 16/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.2563 - loss: 1.3884 - val_accuracy: 0.2959 - val_loss: 1.3799 - learning_rate: 0.0010\n",
      "Epoch 17/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.3268 - loss: 1.3697 - val_accuracy: 0.2832 - val_loss: 1.3758 - learning_rate: 0.0010\n",
      "Epoch 18/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.2892 - loss: 1.3811 - val_accuracy: 0.2959 - val_loss: 1.3707 - learning_rate: 0.0010\n",
      "Epoch 19/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3174 - loss: 1.3658 - val_accuracy: 0.3138 - val_loss: 1.3706 - learning_rate: 0.0010\n",
      "Epoch 20/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.3044 - loss: 1.3664 - val_accuracy: 0.2883 - val_loss: 1.3734 - learning_rate: 0.0010\n",
      "Epoch 21/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.3171 - loss: 1.3613 - val_accuracy: 0.3061 - val_loss: 1.3655 - learning_rate: 0.0010\n",
      "Epoch 22/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.3185 - loss: 1.3564 - val_accuracy: 0.2985 - val_loss: 1.3678 - learning_rate: 0.0010\n",
      "Epoch 23/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3401 - loss: 1.3530 - val_accuracy: 0.3163 - val_loss: 1.3635 - learning_rate: 0.0010\n",
      "Epoch 24/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3399 - loss: 1.3536 - val_accuracy: 0.3469 - val_loss: 1.3591 - learning_rate: 0.0010\n",
      "Epoch 25/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.3254 - loss: 1.3590 - val_accuracy: 0.3597 - val_loss: 1.3550 - learning_rate: 0.0010\n",
      "Epoch 26/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.3355 - loss: 1.3531 - val_accuracy: 0.3163 - val_loss: 1.3628 - learning_rate: 0.0010\n",
      "Epoch 27/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.3544 - loss: 1.3275 - val_accuracy: 0.3010 - val_loss: 1.3595 - learning_rate: 0.0010\n",
      "Epoch 28/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3391 - loss: 1.3466 - val_accuracy: 0.3342 - val_loss: 1.3467 - learning_rate: 0.0010\n",
      "Epoch 29/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3487 - loss: 1.3211 - val_accuracy: 0.3469 - val_loss: 1.3476 - learning_rate: 0.0010\n",
      "Epoch 30/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3362 - loss: 1.3367 - val_accuracy: 0.2959 - val_loss: 1.3528 - learning_rate: 0.0010\n",
      "Epoch 31/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.3502 - loss: 1.3368 - val_accuracy: 0.3699 - val_loss: 1.3381 - learning_rate: 0.0010\n",
      "Epoch 32/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3157 - loss: 1.3551 - val_accuracy: 0.3622 - val_loss: 1.3443 - learning_rate: 0.0010\n",
      "Epoch 33/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3630 - loss: 1.3197 - val_accuracy: 0.3418 - val_loss: 1.3467 - learning_rate: 0.0010\n",
      "Epoch 34/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.3501 - loss: 1.3266 - val_accuracy: 0.3520 - val_loss: 1.3435 - learning_rate: 0.0010\n",
      "Epoch 35/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3435 - loss: 1.3338 - val_accuracy: 0.3724 - val_loss: 1.3361 - learning_rate: 0.0010\n",
      "Epoch 36/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3300 - loss: 1.3355 - val_accuracy: 0.3750 - val_loss: 1.3386 - learning_rate: 0.0010\n",
      "Epoch 37/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3374 - loss: 1.3454 - val_accuracy: 0.3418 - val_loss: 1.3489 - learning_rate: 0.0010\n",
      "Epoch 38/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.3252 - loss: 1.3329 - val_accuracy: 0.3622 - val_loss: 1.3465 - learning_rate: 0.0010\n",
      "Epoch 39/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3663 - loss: 1.3104 - val_accuracy: 0.3622 - val_loss: 1.3371 - learning_rate: 0.0010\n",
      "Epoch 40/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4035 - loss: 1.2855\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.4033 - loss: 1.2859 - val_accuracy: 0.3265 - val_loss: 1.3443 - learning_rate: 0.0010\n",
      "Epoch 41/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.4085 - loss: 1.2931 - val_accuracy: 0.3520 - val_loss: 1.3368 - learning_rate: 5.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3737 - loss: 1.3005 - val_accuracy: 0.3903 - val_loss: 1.3274 - learning_rate: 5.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3687 - loss: 1.3193 - val_accuracy: 0.3750 - val_loss: 1.3276 - learning_rate: 5.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3961 - loss: 1.2941 - val_accuracy: 0.3878 - val_loss: 1.3216 - learning_rate: 5.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3973 - loss: 1.3086 - val_accuracy: 0.3954 - val_loss: 1.3236 - learning_rate: 5.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3864 - loss: 1.2970 - val_accuracy: 0.4056 - val_loss: 1.3221 - learning_rate: 5.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3916 - loss: 1.2878 - val_accuracy: 0.3827 - val_loss: 1.3257 - learning_rate: 5.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.3899 - loss: 1.2852 - val_accuracy: 0.3801 - val_loss: 1.3231 - learning_rate: 5.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3933 - loss: 1.2897\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.3931 - loss: 1.2897 - val_accuracy: 0.3776 - val_loss: 1.3216 - learning_rate: 5.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.4168 - loss: 1.2760 - val_accuracy: 0.3903 - val_loss: 1.3224 - learning_rate: 2.5000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.3914 - loss: 1.2838 - val_accuracy: 0.3622 - val_loss: 1.3251 - learning_rate: 2.5000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - accuracy: 0.3927 - loss: 1.2856 - val_accuracy: 0.3750 - val_loss: 1.3291 - learning_rate: 2.5000e-04\n",
      "Epoch 53/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.4118 - loss: 1.2600 - val_accuracy: 0.3750 - val_loss: 1.3244 - learning_rate: 2.5000e-04\n",
      "Epoch 54/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3791 - loss: 1.2924 - val_accuracy: 0.3954 - val_loss: 1.3196 - learning_rate: 2.5000e-04\n",
      "Epoch 55/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3979 - loss: 1.2907 - val_accuracy: 0.3903 - val_loss: 1.3213 - learning_rate: 2.5000e-04\n",
      "Epoch 56/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3815 - loss: 1.2913 - val_accuracy: 0.4005 - val_loss: 1.3171 - learning_rate: 2.5000e-04\n",
      "Epoch 57/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.3955 - loss: 1.2832 - val_accuracy: 0.3980 - val_loss: 1.3187 - learning_rate: 2.5000e-04\n",
      "Epoch 58/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.4312 - loss: 1.2570 - val_accuracy: 0.3954 - val_loss: 1.3180 - learning_rate: 2.5000e-04\n",
      "Epoch 59/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.4117 - loss: 1.2751 - val_accuracy: 0.3954 - val_loss: 1.3166 - learning_rate: 2.5000e-04\n",
      "Epoch 60/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.4550 - loss: 1.2500 - val_accuracy: 0.3827 - val_loss: 1.3181 - learning_rate: 2.5000e-04\n",
      "Epoch 61/150\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.4086 - loss: 1.2762 - val_accuracy: 0.4031 - val_loss: 1.3150 - learning_rate: 2.5000e-04\n",
      "\n",
      "✅ Test Accuracy: 38.16%\n"
     ]
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------- Filtering Functions ----------------------\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    return butter(order, [low, high], btype='band')\n",
    "\n",
    "def bandpass_filter_data(data, lowcut=8, highcut=30, fs=250):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# ---------------------- EEGNet Model ----------------------\n",
    "def EEGNet(\n",
    "    nb_classes,\n",
    "    Chans=22,\n",
    "    Samples=1000,\n",
    "    dropoutRate1=0.3,\n",
    "    dropoutRate2=0.4,\n",
    "    kernLength=64,\n",
    "    F1=8,\n",
    "    D=2,\n",
    "    F2=16,\n",
    "    norm_rate=0.25\n",
    "):\n",
    "    input1 = Input(shape=(Chans, Samples, 1), name='Input')\n",
    "\n",
    "    # Block 1: Temporal convolution\n",
    "    block1 = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, name='Conv2D_Temporal')(input1)\n",
    "    block1 = BatchNormalization(name='BN_Temporal')(block1)\n",
    "\n",
    "    # Block 1: Spatial filtering\n",
    "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False, depth_multiplier=D,\n",
    "                             depthwise_regularizer=l2(0.001), padding='valid', name='DepthwiseConv2D_Spatial')(block1)\n",
    "    block1 = BatchNormalization(name='BN_Spatial')(block1)\n",
    "    block1 = Activation('relu', name='ReLU_Spatial')(block1)\n",
    "    block1 = AveragePooling2D((1, 4), name='AvgPool1')(block1)\n",
    "    block1 = Dropout(dropoutRate1, name='Dropout1')(block1)\n",
    "\n",
    "    # Block 2: Depthwise Separable Conv\n",
    "    block2 = SeparableConv2D(F2, (1, 8), use_bias=False, padding='same', name='SeparableConv2D')(block1)\n",
    "    block2 = BatchNormalization(name='BN_Separable')(block2)\n",
    "    block2 = Activation('relu', name='ReLU_Separable')(block2)\n",
    "    block2 = AveragePooling2D((1, 8), name='AvgPool2')(block2)\n",
    "    block2 = Dropout(dropoutRate2, name='Dropout2')(block2)\n",
    "\n",
    "    # Classification\n",
    "    flatten = Flatten(name='Flatten')(block2)\n",
    "    dense = Dense(nb_classes, activation='softmax',\n",
    "                  kernel_regularizer=l2(0.001),\n",
    "                  name='Classifier')(flatten)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense, name='EEGNet_Improved')\n",
    "    return model\n",
    "\n",
    "# ---------------------- Load and Preprocess Data ----------------------\n",
    "df = pd.read_csv(r\"E:\\sem8\\Final\\NEW_TRY\\BCICIV_2a_all_patients_normalized1.csv\")\n",
    "\n",
    "eeg_channels = [col for col in df.columns if col.startswith('EEG-')]\n",
    "grouped = df.groupby(['patient', 'epoch'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "label_map = {'left': 0, 'right': 1, 'foot': 2, 'tongue': 3}\n",
    "fs = 250\n",
    "target_length = 500  # Reduced for better efficiency\n",
    "\n",
    "for (patient, epoch), group in grouped:\n",
    "    group_sorted = group.sort_values(by='time')\n",
    "    data = group_sorted[eeg_channels].values\n",
    "\n",
    "    if data.shape[0] < target_length:\n",
    "        data = np.pad(data, ((0, target_length - data.shape[0]), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        data = data[:target_length]\n",
    "\n",
    "    # Bandpass filter each trial\n",
    "    data = bandpass_filter_data(data, 8, 30, fs)\n",
    "\n",
    "    # Z-score normalization per channel\n",
    "    data = (data - np.mean(data, axis=0)) / (np.std(data, axis=0) + 1e-6)\n",
    "\n",
    "    X.append(data)\n",
    "    y.append(label_map[group_sorted['label'].iloc[0]])\n",
    "\n",
    "X = np.array(X)  # (samples, time, channels)\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Reshape: (samples, channels, time, 1)\n",
    "X = np.transpose(X, (0, 2, 1))\n",
    "X = X[..., np.newaxis]\n",
    "\n",
    "# ---------------------- Train-Test Split ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=np.argmax(y, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------- Compute Class Weights ----------------------\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# ---------------------- Train EEGNet ----------------------\n",
    "model = EEGNet(nb_classes=4, Chans=22, Samples=target_length)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=150,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------- Evaluate ----------------------\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\n✅ Test Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb22018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python365\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "49/49 - 10s - 203ms/step - accuracy: 0.2337 - loss: 1.5563 - val_accuracy: 0.3061 - val_loss: 1.3922\n",
      "Epoch 2/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.2676 - loss: 1.4548 - val_accuracy: 0.3061 - val_loss: 1.3908\n",
      "Epoch 3/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.2605 - loss: 1.4424 - val_accuracy: 0.3061 - val_loss: 1.3896\n",
      "Epoch 4/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.2835 - loss: 1.4194 - val_accuracy: 0.3061 - val_loss: 1.3867\n",
      "Epoch 5/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.3167 - loss: 1.3847 - val_accuracy: 0.3061 - val_loss: 1.3809\n",
      "Epoch 6/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.3263 - loss: 1.3711 - val_accuracy: 0.3189 - val_loss: 1.3724\n",
      "Epoch 7/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.3404 - loss: 1.3569 - val_accuracy: 0.3214 - val_loss: 1.3647\n",
      "Epoch 8/50\n",
      "49/49 - 6s - 117ms/step - accuracy: 0.3608 - loss: 1.3411 - val_accuracy: 0.3597 - val_loss: 1.3545\n",
      "Epoch 9/50\n",
      "49/49 - 6s - 117ms/step - accuracy: 0.3761 - loss: 1.3218 - val_accuracy: 0.3469 - val_loss: 1.3366\n",
      "Epoch 10/50\n",
      "49/49 - 6s - 117ms/step - accuracy: 0.3793 - loss: 1.3168 - val_accuracy: 0.3546 - val_loss: 1.3343\n",
      "Epoch 11/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.3921 - loss: 1.3212 - val_accuracy: 0.3724 - val_loss: 1.3318\n",
      "Epoch 12/50\n",
      "49/49 - 6s - 119ms/step - accuracy: 0.4093 - loss: 1.2953 - val_accuracy: 0.3724 - val_loss: 1.3150\n",
      "Epoch 13/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.3857 - loss: 1.3077 - val_accuracy: 0.4362 - val_loss: 1.2901\n",
      "Epoch 14/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4017 - loss: 1.2990 - val_accuracy: 0.4005 - val_loss: 1.2888\n",
      "Epoch 15/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.4138 - loss: 1.2868 - val_accuracy: 0.3801 - val_loss: 1.2872\n",
      "Epoch 16/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4285 - loss: 1.2741 - val_accuracy: 0.4337 - val_loss: 1.2795\n",
      "Epoch 17/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4157 - loss: 1.2777 - val_accuracy: 0.3776 - val_loss: 1.3085\n",
      "Epoch 18/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4310 - loss: 1.2679 - val_accuracy: 0.4031 - val_loss: 1.2743\n",
      "Epoch 19/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4342 - loss: 1.2631 - val_accuracy: 0.4005 - val_loss: 1.2757\n",
      "Epoch 20/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.4610 - loss: 1.2464 - val_accuracy: 0.4362 - val_loss: 1.2737\n",
      "Epoch 21/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4291 - loss: 1.2552 - val_accuracy: 0.4082 - val_loss: 1.2732\n",
      "Epoch 22/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4464 - loss: 1.2444 - val_accuracy: 0.4056 - val_loss: 1.2846\n",
      "Epoch 23/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4406 - loss: 1.2483 - val_accuracy: 0.4515 - val_loss: 1.2258\n",
      "Epoch 24/50\n",
      "49/49 - 6s - 117ms/step - accuracy: 0.4266 - loss: 1.2422 - val_accuracy: 0.4413 - val_loss: 1.2319\n",
      "Epoch 25/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4540 - loss: 1.2315 - val_accuracy: 0.4490 - val_loss: 1.2348\n",
      "Epoch 26/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.4770 - loss: 1.2178 - val_accuracy: 0.4541 - val_loss: 1.2284\n",
      "Epoch 27/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4649 - loss: 1.2141 - val_accuracy: 0.4592 - val_loss: 1.2354\n",
      "Epoch 28/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4681 - loss: 1.2158 - val_accuracy: 0.4566 - val_loss: 1.2182\n",
      "Epoch 29/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4764 - loss: 1.1926 - val_accuracy: 0.4439 - val_loss: 1.2294\n",
      "Epoch 30/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4585 - loss: 1.2114 - val_accuracy: 0.4617 - val_loss: 1.2147\n",
      "Epoch 31/50\n",
      "49/49 - 5s - 112ms/step - accuracy: 0.4693 - loss: 1.2100 - val_accuracy: 0.4668 - val_loss: 1.2121\n",
      "Epoch 32/50\n",
      "49/49 - 5s - 112ms/step - accuracy: 0.4642 - loss: 1.1917 - val_accuracy: 0.4592 - val_loss: 1.2081\n",
      "Epoch 33/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.4630 - loss: 1.2016 - val_accuracy: 0.4643 - val_loss: 1.2134\n",
      "Epoch 34/50\n",
      "49/49 - 5s - 111ms/step - accuracy: 0.4732 - loss: 1.1929 - val_accuracy: 0.4770 - val_loss: 1.2009\n",
      "Epoch 35/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4745 - loss: 1.1992 - val_accuracy: 0.4413 - val_loss: 1.2131\n",
      "Epoch 36/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4949 - loss: 1.1724 - val_accuracy: 0.4694 - val_loss: 1.1967\n",
      "Epoch 37/50\n",
      "49/49 - 5s - 112ms/step - accuracy: 0.4693 - loss: 1.1988 - val_accuracy: 0.4745 - val_loss: 1.2087\n",
      "Epoch 38/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4738 - loss: 1.1940 - val_accuracy: 0.4770 - val_loss: 1.1906\n",
      "Epoch 39/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4891 - loss: 1.1647 - val_accuracy: 0.4643 - val_loss: 1.1868\n",
      "Epoch 40/50\n",
      "49/49 - 6s - 115ms/step - accuracy: 0.4630 - loss: 1.2055 - val_accuracy: 0.4592 - val_loss: 1.1959\n",
      "Epoch 41/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4745 - loss: 1.1959 - val_accuracy: 0.4898 - val_loss: 1.1743\n",
      "Epoch 42/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4917 - loss: 1.1776 - val_accuracy: 0.4821 - val_loss: 1.1919\n",
      "Epoch 43/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4911 - loss: 1.1596 - val_accuracy: 0.4592 - val_loss: 1.1841\n",
      "Epoch 44/50\n",
      "49/49 - 6s - 113ms/step - accuracy: 0.4713 - loss: 1.1776 - val_accuracy: 0.4694 - val_loss: 1.1950\n",
      "Epoch 45/50\n",
      "49/49 - 6s - 124ms/step - accuracy: 0.4789 - loss: 1.1758 - val_accuracy: 0.4694 - val_loss: 1.1977\n",
      "Epoch 46/50\n",
      "49/49 - 6s - 119ms/step - accuracy: 0.5006 - loss: 1.1593 - val_accuracy: 0.4898 - val_loss: 1.1789\n",
      "Epoch 47/50\n",
      "49/49 - 6s - 114ms/step - accuracy: 0.4853 - loss: 1.1697 - val_accuracy: 0.4694 - val_loss: 1.1866\n",
      "Epoch 48/50\n",
      "49/49 - 5s - 112ms/step - accuracy: 0.4930 - loss: 1.1672 - val_accuracy: 0.4949 - val_loss: 1.1741\n",
      "Epoch 49/50\n",
      "49/49 - 6s - 119ms/step - accuracy: 0.4962 - loss: 1.1759 - val_accuracy: 0.4847 - val_loss: 1.1717\n",
      "Epoch 50/50\n",
      "49/49 - 6s - 116ms/step - accuracy: 0.4879 - loss: 1.1742 - val_accuracy: 0.4796 - val_loss: 1.1901\n",
      "Test Accuracy: 51.02%\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57       138\n",
      "           1       0.44      0.66      0.53       126\n",
      "           2       0.58      0.28      0.38       118\n",
      "           3       0.55      0.48      0.51       108\n",
      "\n",
      "    accuracy                           0.51       490\n",
      "   macro avg       0.53      0.50      0.50       490\n",
      "weighted avg       0.53      0.51      0.50       490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def EEGNet(nb_classes, Chans=22, Samples=1000, dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25):\n",
    "    input1 = Input(shape=(Chans, Samples, 1))\n",
    "\n",
    "    block1 = Conv2D(F1, (1, kernLength), padding='same',\n",
    "                    input_shape=(Chans, Samples, 1),\n",
    "                    use_bias=False)(input1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False,\n",
    "                             depth_multiplier=D,\n",
    "                             depthwise_regularizer=l2(0.001),\n",
    "                             padding='valid')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = AveragePooling2D((1, 4))(block1)\n",
    "    block1 = Dropout(dropoutRate)(block1)\n",
    "\n",
    "    block2 = SeparableConv2D(F2, (1, 16),\n",
    "                             use_bias=False, padding='same')(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((1, 8))(block2)\n",
    "    block2 = Dropout(dropoutRate)(block2)\n",
    "\n",
    "    flatten = Flatten()(block2)\n",
    "\n",
    "    dense = Dense(nb_classes, activation='softmax',\n",
    "                  kernel_regularizer=l2(0.001))(flatten)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "Chans = 22\n",
    "Samples = 1000\n",
    "nb_classes = 4\n",
    "num_samples = 1000  # Total EEG epochs/samples\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"E:\\sem8\\Final\\NEW_TRY\\BCICIV_2a_all_patients_normalized1.csv\")\n",
    "\n",
    "# Extract EEG channels\n",
    "eeg_channels = [col for col in df.columns if col.startswith('EEG-')]\n",
    "grouped = df.groupby(['patient', 'epoch'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "label_map = {'left': 0, 'right': 1, 'foot': 2, 'tongue': 3}\n",
    "\n",
    "for (patient, epoch), group in grouped:\n",
    "    group_sorted = group.sort_values(by='time')\n",
    "    data = group_sorted[eeg_channels].values\n",
    "    if data.shape[0] < 1000:  # pad if needed\n",
    "        data = np.pad(data, ((0, 1000 - data.shape[0]), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        data = data[:1000]  # crop\n",
    "    X.append(data)\n",
    "    label = group_sorted['label'].iloc[0]\n",
    "    y.append(label_map[label])\n",
    "\n",
    "X = np.array(X)  # shape: (n_trials, time_steps, 22)\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Reshape for EEGNet: (n_samples, channels, time, 1)\n",
    "X = np.transpose(X, (0, 2, 1))  # (samples, channels, time)\n",
    "X = X[..., np.newaxis]          # (samples, channels, time, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = EEGNet(nb_classes=nb_classes, Chans=Chans, Samples=Samples)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# Prediction & classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.02%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
