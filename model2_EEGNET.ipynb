{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cbd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, AveragePooling2D,\n",
    "    Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def EEGNet(nb_classes, Chans=22, Samples=1000, dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25):\n",
    "    input1 = Input(shape=(Chans, Samples, 1))\n",
    "\n",
    "    block1 = Conv2D(F1, (1, kernLength), padding='same',\n",
    "                    input_shape=(Chans, Samples, 1),\n",
    "                    use_bias=False)(input1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((Chans, 1), use_bias=False,\n",
    "                             depth_multiplier=D,\n",
    "                             depthwise_regularizer=l2(0.001),\n",
    "                             padding='valid')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = AveragePooling2D((1, 4))(block1)\n",
    "    block1 = Dropout(dropoutRate)(block1)\n",
    "\n",
    "    block2 = SeparableConv2D(F2, (1, 16),\n",
    "                             use_bias=False, padding='same')(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((1, 8))(block2)\n",
    "    block2 = Dropout(dropoutRate)(block2)\n",
    "\n",
    "    flatten = Flatten()(block2)\n",
    "\n",
    "    dense = Dense(nb_classes, activation='softmax',\n",
    "                  kernel_regularizer=l2(0.001))(flatten)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44f4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"E:\\sem8\\Final\\NEW_TRY\\BCICIV_2a_all_patients_normalized1.csv\")\n",
    "\n",
    "# Extract EEG channels\n",
    "eeg_channels = [col for col in df.columns if col.startswith('EEG-')]\n",
    "grouped = df.groupby(['patient', 'epoch'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "label_map = {'left': 0, 'right': 1, 'foot': 2, 'tongue': 3}\n",
    "\n",
    "for (patient, epoch), group in grouped:\n",
    "    group_sorted = group.sort_values(by='time')\n",
    "    data = group_sorted[eeg_channels].values\n",
    "    if data.shape[0] < 1000:  # pad if needed\n",
    "        data = np.pad(data, ((0, 1000 - data.shape[0]), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        data = data[:1000]  # crop\n",
    "    X.append(data)\n",
    "    label = group_sorted['label'].iloc[0]\n",
    "    y.append(label_map[label])\n",
    "\n",
    "X = np.array(X)  # shape: (n_trials, time_steps, 22)\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Reshape for EEGNet: (n_samples, channels, time, 1)\n",
    "X = np.transpose(X, (0, 2, 1))  # (samples, channels, time)\n",
    "X = X[..., np.newaxis]          # (samples, channels, time, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y.argmax(1), random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e250bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python365\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.2618 - loss: 1.6313 - val_accuracy: 0.2347 - val_loss: 1.3959\n",
      "Epoch 2/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.2640 - loss: 1.4912 - val_accuracy: 0.2857 - val_loss: 1.3938\n",
      "Epoch 3/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.2758 - loss: 1.4583 - val_accuracy: 0.2857 - val_loss: 1.3931\n",
      "Epoch 4/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.2452 - loss: 1.4568 - val_accuracy: 0.2857 - val_loss: 1.3931\n",
      "Epoch 5/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.2840 - loss: 1.4237 - val_accuracy: 0.3138 - val_loss: 1.3927\n",
      "Epoch 6/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.2488 - loss: 1.4222 - val_accuracy: 0.3036 - val_loss: 1.3902\n",
      "Epoch 7/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.2722 - loss: 1.3969 - val_accuracy: 0.2832 - val_loss: 1.3828\n",
      "Epoch 8/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.2654 - loss: 1.4121 - val_accuracy: 0.3112 - val_loss: 1.3814\n",
      "Epoch 9/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.2995 - loss: 1.3845 - val_accuracy: 0.3444 - val_loss: 1.3618\n",
      "Epoch 10/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3054 - loss: 1.3620 - val_accuracy: 0.3240 - val_loss: 1.3502\n",
      "Epoch 11/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3199 - loss: 1.3557 - val_accuracy: 0.3597 - val_loss: 1.3374\n",
      "Epoch 12/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3341 - loss: 1.3555 - val_accuracy: 0.3673 - val_loss: 1.3204\n",
      "Epoch 13/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3377 - loss: 1.3226 - val_accuracy: 0.3673 - val_loss: 1.3097\n",
      "Epoch 14/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 119ms/step - accuracy: 0.3700 - loss: 1.3311 - val_accuracy: 0.4133 - val_loss: 1.2964\n",
      "Epoch 15/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3760 - loss: 1.3110 - val_accuracy: 0.4056 - val_loss: 1.2957\n",
      "Epoch 16/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.3712 - loss: 1.3204 - val_accuracy: 0.4260 - val_loss: 1.3008\n",
      "Epoch 17/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4129 - loss: 1.2788 - val_accuracy: 0.3954 - val_loss: 1.3002\n",
      "Epoch 18/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.3964 - loss: 1.3002 - val_accuracy: 0.4031 - val_loss: 1.2899\n",
      "Epoch 19/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4115 - loss: 1.2762 - val_accuracy: 0.4056 - val_loss: 1.2900\n",
      "Epoch 20/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4254 - loss: 1.2462 - val_accuracy: 0.4209 - val_loss: 1.2877\n",
      "Epoch 21/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4160 - loss: 1.2480 - val_accuracy: 0.4413 - val_loss: 1.2637\n",
      "Epoch 22/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4354 - loss: 1.2414 - val_accuracy: 0.4362 - val_loss: 1.2664\n",
      "Epoch 23/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4577 - loss: 1.2337 - val_accuracy: 0.4184 - val_loss: 1.2738\n",
      "Epoch 24/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.4195 - loss: 1.2518 - val_accuracy: 0.4311 - val_loss: 1.2597\n",
      "Epoch 25/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.4351 - loss: 1.2439 - val_accuracy: 0.4464 - val_loss: 1.2417\n",
      "Epoch 26/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4631 - loss: 1.2047 - val_accuracy: 0.4490 - val_loss: 1.2474\n",
      "Epoch 27/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.4423 - loss: 1.2205 - val_accuracy: 0.4643 - val_loss: 1.2361\n",
      "Epoch 28/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4450 - loss: 1.2312 - val_accuracy: 0.4566 - val_loss: 1.2462\n",
      "Epoch 29/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4330 - loss: 1.2199 - val_accuracy: 0.4847 - val_loss: 1.2192\n",
      "Epoch 30/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4618 - loss: 1.1824 - val_accuracy: 0.5000 - val_loss: 1.2306\n",
      "Epoch 31/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4800 - loss: 1.1851 - val_accuracy: 0.4898 - val_loss: 1.2308\n",
      "Epoch 32/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4360 - loss: 1.2169 - val_accuracy: 0.4719 - val_loss: 1.2174\n",
      "Epoch 33/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4715 - loss: 1.2091 - val_accuracy: 0.4796 - val_loss: 1.2288\n",
      "Epoch 34/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4878 - loss: 1.1849 - val_accuracy: 0.4949 - val_loss: 1.2180\n",
      "Epoch 35/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4591 - loss: 1.1882 - val_accuracy: 0.4949 - val_loss: 1.2092\n",
      "Epoch 36/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4836 - loss: 1.2019 - val_accuracy: 0.4974 - val_loss: 1.2153\n",
      "Epoch 37/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4673 - loss: 1.1839 - val_accuracy: 0.4770 - val_loss: 1.2187\n",
      "Epoch 38/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4878 - loss: 1.1741 - val_accuracy: 0.5000 - val_loss: 1.2075\n",
      "Epoch 39/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4811 - loss: 1.1717 - val_accuracy: 0.4872 - val_loss: 1.2052\n",
      "Epoch 40/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.4581 - loss: 1.1975 - val_accuracy: 0.4796 - val_loss: 1.2046\n",
      "Epoch 41/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 128ms/step - accuracy: 0.5059 - loss: 1.1737 - val_accuracy: 0.4974 - val_loss: 1.2021\n",
      "Epoch 42/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4823 - loss: 1.1808 - val_accuracy: 0.4949 - val_loss: 1.2015\n",
      "Epoch 43/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.5061 - loss: 1.1688 - val_accuracy: 0.5051 - val_loss: 1.1889\n",
      "Epoch 44/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.4661 - loss: 1.1786 - val_accuracy: 0.4949 - val_loss: 1.2013\n",
      "Epoch 45/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4509 - loss: 1.2046 - val_accuracy: 0.5102 - val_loss: 1.1943\n",
      "Epoch 46/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4770 - loss: 1.1869 - val_accuracy: 0.4872 - val_loss: 1.1985\n",
      "Epoch 47/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.5032 - loss: 1.1303 - val_accuracy: 0.4949 - val_loss: 1.1895\n",
      "Epoch 48/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4829 - loss: 1.1845 - val_accuracy: 0.4898 - val_loss: 1.1911\n",
      "Epoch 49/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5163 - loss: 1.1316 - val_accuracy: 0.4923 - val_loss: 1.1923\n",
      "Epoch 50/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.5087 - loss: 1.1550 - val_accuracy: 0.4847 - val_loss: 1.1950\n",
      "Epoch 51/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5053 - loss: 1.1641 - val_accuracy: 0.4949 - val_loss: 1.1818\n",
      "Epoch 52/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4753 - loss: 1.1611 - val_accuracy: 0.5128 - val_loss: 1.1886\n",
      "Epoch 53/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4790 - loss: 1.1666 - val_accuracy: 0.5077 - val_loss: 1.1774\n",
      "Epoch 54/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.4753 - loss: 1.1826 - val_accuracy: 0.5179 - val_loss: 1.1895\n",
      "Epoch 55/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.5102 - loss: 1.1421 - val_accuracy: 0.5255 - val_loss: 1.1752\n",
      "Epoch 56/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5214 - loss: 1.1441 - val_accuracy: 0.5281 - val_loss: 1.1722\n",
      "Epoch 57/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4743 - loss: 1.1834 - val_accuracy: 0.5077 - val_loss: 1.1854\n",
      "Epoch 58/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4653 - loss: 1.1925 - val_accuracy: 0.5128 - val_loss: 1.1780\n",
      "Epoch 59/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4824 - loss: 1.1845 - val_accuracy: 0.5281 - val_loss: 1.1782\n",
      "Epoch 60/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4833 - loss: 1.1541 - val_accuracy: 0.5230 - val_loss: 1.1753\n",
      "Epoch 61/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5094 - loss: 1.1448 - val_accuracy: 0.5000 - val_loss: 1.1910\n",
      "Epoch 62/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.4830 - loss: 1.1661 - val_accuracy: 0.5255 - val_loss: 1.1704\n",
      "Epoch 63/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5035 - loss: 1.1722 - val_accuracy: 0.5153 - val_loss: 1.1814\n",
      "Epoch 64/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.5162 - loss: 1.1464 - val_accuracy: 0.5204 - val_loss: 1.1801\n",
      "Epoch 65/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.4989 - loss: 1.1507 - val_accuracy: 0.5153 - val_loss: 1.1832\n",
      "Epoch 66/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.5016 - loss: 1.1558 - val_accuracy: 0.5077 - val_loss: 1.1795\n",
      "Epoch 67/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.5091 - loss: 1.1468 - val_accuracy: 0.5179 - val_loss: 1.1736\n",
      "Epoch 68/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.5072 - loss: 1.1627 - val_accuracy: 0.5230 - val_loss: 1.1704\n",
      "Epoch 69/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.4934 - loss: 1.1395 - val_accuracy: 0.5332 - val_loss: 1.1736\n",
      "Epoch 70/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - accuracy: 0.4797 - loss: 1.1822 - val_accuracy: 0.5000 - val_loss: 1.1857\n",
      "Epoch 71/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.4985 - loss: 1.1447 - val_accuracy: 0.5179 - val_loss: 1.1781\n",
      "Epoch 72/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.4836 - loss: 1.1688 - val_accuracy: 0.5153 - val_loss: 1.1702\n",
      "Epoch 73/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.4907 - loss: 1.1620 - val_accuracy: 0.5281 - val_loss: 1.1652\n",
      "Epoch 74/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.4707 - loss: 1.1745 - val_accuracy: 0.5255 - val_loss: 1.1731\n",
      "Epoch 75/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - accuracy: 0.4696 - loss: 1.1940 - val_accuracy: 0.5204 - val_loss: 1.1758\n",
      "Epoch 76/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.5143 - loss: 1.1409 - val_accuracy: 0.5230 - val_loss: 1.1635\n",
      "Epoch 77/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.4878 - loss: 1.1534 - val_accuracy: 0.5485 - val_loss: 1.1554\n",
      "Epoch 78/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.4987 - loss: 1.1418 - val_accuracy: 0.5000 - val_loss: 1.1859\n",
      "Epoch 79/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.5010 - loss: 1.1429 - val_accuracy: 0.5306 - val_loss: 1.1801\n",
      "Epoch 80/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.5103 - loss: 1.1563 - val_accuracy: 0.5179 - val_loss: 1.1631\n",
      "Epoch 81/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.4839 - loss: 1.1232 - val_accuracy: 0.5179 - val_loss: 1.1679\n",
      "Epoch 82/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.4745 - loss: 1.1434 - val_accuracy: 0.5434 - val_loss: 1.1717\n",
      "Epoch 83/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.5154 - loss: 1.1395 - val_accuracy: 0.5485 - val_loss: 1.1556\n",
      "Epoch 84/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.4981 - loss: 1.1384 - val_accuracy: 0.5332 - val_loss: 1.1595\n",
      "Epoch 85/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.5288 - loss: 1.1308 - val_accuracy: 0.5357 - val_loss: 1.1634\n",
      "Epoch 86/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.5031 - loss: 1.1445 - val_accuracy: 0.5255 - val_loss: 1.1695\n",
      "Epoch 87/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - accuracy: 0.5013 - loss: 1.1334 - val_accuracy: 0.5306 - val_loss: 1.1576\n",
      "Epoch 88/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - accuracy: 0.5130 - loss: 1.1350 - val_accuracy: 0.5357 - val_loss: 1.1569\n",
      "Epoch 89/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 135ms/step - accuracy: 0.5069 - loss: 1.1461 - val_accuracy: 0.5102 - val_loss: 1.1715\n",
      "Epoch 90/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.4809 - loss: 1.1528 - val_accuracy: 0.5332 - val_loss: 1.1522\n",
      "Epoch 91/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - accuracy: 0.5195 - loss: 1.1232 - val_accuracy: 0.5357 - val_loss: 1.1517\n",
      "Epoch 92/200\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - accuracy: 0.4846 - loss: 1.1593 - val_accuracy: 0.5179 - val_loss: 1.1670\n",
      "Test Accuracy: 54.29%\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet(nb_classes=4, Chans=22, Samples=1000)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(1e-3), metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
